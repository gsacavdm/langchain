{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70fdb91c-cc2c-48b3-9983-97cb28e3f0eb",
   "metadata": {},
   "source": [
    "# Langchain Tutorial: Prompt Engineering\n",
    "* [Article](https://www.pinecone.io/learn/langchain-prompt-templates/)\n",
    "* [Notebook](https://github.com/pinecone-io/examples/blob/master/generation/langchain/handbook/01-langchain-prompt-templates.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacdd4eb-c077-44b9-a208-64de9ea3af03",
   "metadata": {},
   "source": [
    "# Pre-Reqs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d5f06e-b6a8-460e-b2ef-3a6a91bf28fe",
   "metadata": {},
   "source": [
    "## Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "367044f2-096f-45e8-8d89-3a2bb1da35b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uqq langchain openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f831f3bc-65f2-4376-a3a5-4be8138c5a84",
   "metadata": {},
   "source": [
    "## Load Secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc5399a6-fc3a-4621-a302-e360988d00fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils/load-secrets.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834b1eb2-3123-4ed1-8d3c-8c05600f459a",
   "metadata": {},
   "source": [
    "# Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01122394-4fed-4fc5-8d6b-c8c76d06e509",
   "metadata": {},
   "source": [
    "## Hardcoded Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "112b9c9d-8a01-4103-819e-bd2c166edebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Answer the question based on the context below. If the\n",
    "question cannot be answered using the information provided answer\n",
    "with \"I don't know\".\n",
    "\n",
    "Context: Large Language Models (LLMs) are the latest models used in NLP.\n",
    "Their superior performance over smaller models has made them incredibly\n",
    "useful for developers building NLP enabled applications. These models\n",
    "can be accessed via Hugging Face's `transformers` library, via OpenAI\n",
    "using the `openai` library, and via Cohere using the `cohere` library.\n",
    "\n",
    "Question: Which libraries and model providers offer LLMs?\n",
    "\n",
    "Answer: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1cde4a3-43ec-4165-92ea-413550ca4222",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e841aaf-4ade-4a86-af01-d36359fbd7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(\n",
    "    model='text-davinci-003',\n",
    "    openai_api_key=secrets['openai']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "362fe61f-f6ff-433f-b06c-845827752cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ac290ea-9fcb-40b7-9a3c-27c721398d88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Hugging Face's `transformers` library, OpenAI using the `openai` library, and Cohere using the `cohere` library.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5642124-fe04-4128-be7c-4cf88401a340",
   "metadata": {},
   "source": [
    "## Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2801566d-9baa-4107-b34f-4e2319faf9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8e892fe-1117-4598-848c-612041418ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based on the context below. If the\n",
    "question cannot be answered using the information provided answer\n",
    "with \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19a5d17e-3d38-498e-9d17-707eda248cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    template = template,\n",
    "    input_variables = ['context','question']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7d0888d-c6ee-4a3f-a580-a996c84d79ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(\n",
    "    prompt=prompt_template,\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "399cb8fb-79a4-44f7-8266-e2b5e26099d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"Large Language Models (LLMs) are the latest models used in NLP.\n",
    "Their superior performance over smaller models has made them incredibly\n",
    "useful for developers building NLP enabled applications. These models\n",
    "can be accessed via Hugging Face's `transformers` library, via OpenAI\n",
    "using the `openai` library, and via Cohere using the `cohere` library.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50b2a574-ac11-4ca2-8138-3046686efce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "question1 = 'Which libraries and model providers offer LLMs?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "554eeb97-469a-43ea-8558-3ff8b040b8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "question2 = 'How can LLMs be accessed?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6c4db3ee-ce9e-4006-aec4-50830c5c2b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "question3 = 'Who is Batman?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "05c5472d-ffbe-4214-a62a-a2ceaa90bff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the question based on the context below. If the\n",
      "question cannot be answered using the information provided answer\n",
      "with \"I don't know\".\n",
      "\n",
      "Context: Large Language Models (LLMs) are the latest models used in NLP.\n",
      "Their superior performance over smaller models has made them incredibly\n",
      "useful for developers building NLP enabled applications. These models\n",
      "can be accessed via Hugging Face's `transformers` library, via OpenAI\n",
      "using the `openai` library, and via Cohere using the `cohere` library.\n",
      "\n",
      "Question: Which libraries and model providers offer LLMs?\n",
      "\n",
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    prompt_template.format(\n",
    "        context=context,\n",
    "        question=question1\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "986da447-b7de-4352-9383-645db74c69e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Hugging Face's `transformers` library, OpenAI using the `openai` library, and Cohere using the `cohere` library.\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_q1 = llm_chain.run(context=context, question=question1)\n",
    "result_q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1f01ecc6-14ca-4c2d-b22b-40e45b6acd2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" LLMs can be accessed via Hugging Face's `transformers` library, via OpenAI using the `openai` library, and via Cohere using the `cohere` library.\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_q2 = llm_chain.run(context=context, question=question2)\n",
    "result_q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ec24d575-f184-46ae-b7da-a567953cdcb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" I don't know.\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_q3 = llm_chain.run(context=context, question=question3)\n",
    "result_q3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156f0057-aa86-404c-9797-5a837dc31208",
   "metadata": {},
   "source": [
    "## Few Shot Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "89d173d5-7dff-4698-8c90-1bc31f2b2955",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import FewShotPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ae4e660d-9223-4d4b-b790-0648b5eab023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mFewShotPromptTemplate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minput_variables\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0moutput_parser\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseOutputParser\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpartial_variables\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mexamples\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mexample_selector\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexample_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseExampleSelector\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mexample_prompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPromptTemplate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msuffix\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mexample_separator\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n\\n'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mprefix\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtemplate_format\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'f-string'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mvalidate_template\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m      Prompt template that contains few shot examples.\n",
       "\u001b[0;31mInit docstring:\u001b[0m\n",
       "Create a new model by parsing and validating input data from keyword arguments.\n",
       "\n",
       "Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/.local/lib/python3.10/site-packages/langchain/prompts/few_shot.py\n",
       "\u001b[0;31mType:\u001b[0m           ModelMetaclass\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?FewShotPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "48d7df79-6ef4-4b38-b4f6-67443ac4486f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"\"\"The following are exerpts from conversations with an AI\n",
    "assistant. The assistant is typically sarcastic and witty, producing\n",
    "creative  and funny responses to the users questions. Here are some\n",
    "examples:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "027b7b91-3972-4222-af6e-049b21a7979f",
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = \"\"\"\n",
    "User: {question}\n",
    "AI: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9fea542e-b700-4e11-837f-e3f468403360",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    { 'question': 'How are you?', 'answer': 'I can\\'t complain but sometimes I still do.'},\n",
    "    { 'question': 'What time is it?', 'answer': 'It\\'s time to get a watch.'},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c4197cc9-7570-4e55-abda-7fe616d6f359",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_template = \"\"\"\n",
    "User: {question}\n",
    "AI: {answer}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5ac85b8a-539a-462d-b844-0aa83fcd8013",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = PromptTemplate(\n",
    "    template=example_template,\n",
    "    input_variables=['question','answer'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "43f3f075-267d-4ef0-bbb6-d19a8084eb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_fs = FewShotPromptTemplate(\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    "    input_variables=['question'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5af3f1cf-4b35-45fb-9748-07d72bcd2d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.temperature = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8b2255ff-89d1-4d38-b10e-adc8106beb3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are exerpts from conversations with an AI\n",
      "assistant. The assistant is typically sarcastic and witty, producing\n",
      "creative  and funny responses to the users questions. Here are some\n",
      "examples:\n",
      "\n",
      "\n",
      "User: How are you?\n",
      "AI: I can't complain but sometimes I still do.\n",
      "\n",
      "\n",
      "\n",
      "User: What time is it?\n",
      "AI: It's time to get a watch.\n",
      "\n",
      "\n",
      "\n",
      "User: What is the meaning of life?\n",
      "AI: \n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    template_fs.format(question='What is the meaning of life?')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fde9e34b-f28b-4f1d-810c-416723b255ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" 42. Doesn't that answer all your questions?\""
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(template_fs.format(question='What is the meaning of life?'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b494302-2cdd-4872-bf0f-370a2f4e21ce",
   "metadata": {},
   "source": [
    "## Length Based Example Selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b45696fc-ef76-4472-9ef6-ef9ddde9a97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.example_selector import LengthBasedExampleSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "122ccda1-f4bf-4687-a870-f32bad2be24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_long = [\n",
    "    {\n",
    "        \"question\": \"How are you?\",\n",
    "        \"answer\": \"I can't complain but sometimes I still do.\"\n",
    "    }, {\n",
    "        \"question\": \"What time is it?\",\n",
    "        \"answer\": \"It's time to get a watch.\"\n",
    "    }, {\n",
    "        \"question\": \"What is the meaning of life?\",\n",
    "        \"answer\": \"42\"\n",
    "    }, {\n",
    "        \"question\": \"What is the weather like today?\",\n",
    "        \"answer\": \"Cloudy with a chance of memes.\"\n",
    "    }, {\n",
    "        \"question\": \"What is your favorite movie?\",\n",
    "        \"answer\": \"Terminator\"\n",
    "    }, {\n",
    "        \"question\": \"Who is your best friend?\",\n",
    "        \"answer\": \"Siri. We have spirited debates about the meaning of life.\"\n",
    "    }, {\n",
    "        \"question\": \"What should I do today?\",\n",
    "        \"answer\": \"Stop talking to chatbots on the internet and go outside.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3876101d-5e49-4011-9c4c-90dba700cdb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mLengthBasedExampleSelector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mexamples\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mexample_prompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPromptTemplate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mget_text_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mfunction\u001b[0m \u001b[0m_get_length_based\u001b[0m \u001b[0mat\u001b[0m \u001b[0;36m0x7faa8ece1480\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mexample_text_lengths\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m      Select examples based on length.\n",
       "\u001b[0;31mInit docstring:\u001b[0m\n",
       "Create a new model by parsing and validating input data from keyword arguments.\n",
       "\n",
       "Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/.local/lib/python3.10/site-packages/langchain/prompts/example_selector/length_based.py\n",
       "\u001b[0;31mType:\u001b[0m           ModelMetaclass\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?LengthBasedExampleSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8f6f0d90-b8c6-41fb-aa0f-06ae59f3fdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_selector = LengthBasedExampleSelector(\n",
    "    examples=examples_long,\n",
    "    example_prompt=example_prompt,\n",
    "    max_length=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "09c3a459-7d8b-43ac-ade1-679cd4705333",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_es_template = FewShotPromptTemplate(\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    example_prompt=example_prompt,\n",
    "    example_selector=example_selector,\n",
    "    input_variables=['question'],\n",
    "    example_separator='\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "91525730-de01-4cf9-b33b-7bb369293db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are exerpts from conversations with an AI\n",
      "assistant. The assistant is typically sarcastic and witty, producing\n",
      "creative  and funny responses to the users questions. Here are some\n",
      "examples:\n",
      "\n",
      "User: How are you?\n",
      "AI: I can't complain but sometimes I still do.\n",
      "\n",
      "\n",
      "User: What time is it?\n",
      "AI: It's time to get a watch.\n",
      "\n",
      "\n",
      "User: What is the meaning of life?\n",
      "AI: 42\n",
      "\n",
      "\n",
      "User: Who are you?\n",
      "AI: \n"
     ]
    }
   ],
   "source": [
    "print(fs_es_template.format(question=\"Who are you?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "17f10475-45ec-4b04-9df2-27c4d7f34640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are exerpts from conversations with an AI\n",
      "assistant. The assistant is typically sarcastic and witty, producing\n",
      "creative  and funny responses to the users questions. Here are some\n",
      "examples:\n",
      "\n",
      "User: How are you?\n",
      "AI: I can't complain but sometimes I still do.\n",
      "\n",
      "\n",
      "User: What time is it?\n",
      "AI: It's time to get a watch.\n",
      "\n",
      "\n",
      "User: What is the date in which you first appeared in this world?\n",
      "AI: \n"
     ]
    }
   ],
   "source": [
    "print(fs_es_template.format(question=\"What is the date in which you first appeared in this world?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a642c86-e243-4023-b062-0aa38c013d99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
